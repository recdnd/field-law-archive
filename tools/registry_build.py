#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Spiral Registry Builder v2
è¿ç§»å·¥å…·ï¼šä» TXT åˆ° JSON SSOT
"""

import os
import re
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

# ============================================================================
# æ ‡ç‚¹ç¬¦å·æ¸…ç†ï¼ˆ1:1 åŒ¹é… punctuation_cleaner.pyï¼‰
# ============================================================================

def clean_to_english_punctuation(text: str) -> str:
    """
    æ ‡ç‚¹ç¬¦å·æ¸…ç†å‡½æ•°ï¼Œå®Œå…¨åŒ¹é… puncc.py çš„è¡Œä¸º
    å¿…é¡»ä¿ç•™ï¼š
    - ä¸‰é‡åå¼•å·ä»£ç å—
    - URLs (http:// æˆ– https://)
    """
    if not text:
        return text
    
    # ä¿æŠ¤ä»£ç å—ï¼šæå–æ‰€æœ‰ ```...``` å—
    code_blocks = []
    code_block_pattern = r'```[\s\S]*?```'
    
    def replace_code_block(match):
        idx = len(code_blocks)
        code_blocks.append(match.group(0))
        return f"__CODE_BLOCK_{idx}__"
    
    protected_text = re.sub(code_block_pattern, replace_code_block, text)
    
    # ä¿æŠ¤ URLs
    url_pattern = r'https?://[^\s]+'
    urls = []
    
    def replace_url(match):
        idx = len(urls)
        urls.append(match.group(0))
        return f"__URL_{idx}__"
    
    protected_text = re.sub(url_pattern, replace_url, protected_text)
    
    # æ‰§è¡Œæ ‡ç‚¹æ›¿æ¢ï¼ˆä¸ puncc.py å®Œå…¨ä¸€è‡´ï¼‰
    replacements = {
        "ï¼Œ": ", ",
        "ã€‚": ".",
        "ï¼š": ":",
        """: "\"",
        """: "\"",
        "'": "'",
        "'": "'",
        "ã€": ", ",
        "ï¼ˆ": "(",
        "ï¼‰": ")",
        "ã€Š": "<",
        "ã€‹": ">",
        "ã€": "[",
        "ã€‘": "]",
        "ï¼": "!",
        "ï¼Ÿ": "?",
        "ï¼": "/",
        "ï¼›": ";"
    }
    
    for zh, en in replacements.items():
        protected_text = protected_text.replace(zh, en)
    
    # æ¢å¤ URLs
    for idx, url in enumerate(urls):
        protected_text = protected_text.replace(f"__URL_{idx}__", url)
    
    # æ¢å¤ä»£ç å—
    for idx, code_block in enumerate(code_blocks):
        protected_text = protected_text.replace(f"__CODE_BLOCK_{idx}__", code_block)
    
    return protected_text


def sanitize_text(text: str, should_sanitize: bool = True) -> str:
    """åº”ç”¨æ ‡ç‚¹æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if not should_sanitize:
        return text
    return clean_to_english_punctuation(text)


# ============================================================================
# è§£æå™¨ï¼šå®¹å¿æ ¼å¼æ¼‚ç§»
# ============================================================================

def parse_weight(weight_str: str) -> int:
    """
    è§£ææƒé‡ï¼šæ¥å— â˜…â˜…â˜…â˜…â˜…, 5, â˜… â˜… â˜…, *** ç­‰æ ¼å¼
    è¿”å› 1-5 çš„æ•´æ•°
    """
    if not weight_str:
        return 3  # é»˜è®¤
    
    # æå–æ‰€æœ‰æ˜Ÿå·ï¼ˆå…¨è§’/åŠè§’/ASCIIï¼‰
    stars = re.findall(r'[â˜…â˜†*]', weight_str)
    count = len(stars)
    
    # ä¹Ÿå°è¯•æ•°å­—
    num_match = re.search(r'\d+', weight_str)
    if num_match:
        num = int(num_match.group(0))
        if 1 <= num <= 5:
            return num
    
    # æ˜Ÿå·è®¡æ•°
    if 1 <= count <= 5:
        return count
    
    return 3  # é»˜è®¤å€¼


def parse_tags(tags_str: str) -> List[str]:
    """
    è§£ææ ‡ç­¾ï¼šæ¥å—ç©ºæ ¼/é€—å·/æ–œæ åˆ†éš”
    è§„èŒƒåŒ–ï¼šç¡®ä¿ä»¥ # å¼€å¤´
    """
    if not tags_str:
        return []
    
    # åˆ†å‰²ï¼šç©ºæ ¼ã€é€—å·ã€æ–œæ 
    tags = re.split(r'[\s,/#]+', tags_str)
    
    normalized = []
    for tag in tags:
        tag = tag.strip()
        if not tag:
            continue
        
        # ç¡®ä¿ä»¥ # å¼€å¤´
        if not tag.startswith('#'):
            tag = '#' + tag
        
        normalized.append(tag)
    
    return list(set(normalized))  # å»é‡


def normalize_title(title: str) -> str:
    """
    è§„èŒƒåŒ– title æ ¼å¼ä¸ºï¼šä¸­æ–‡ | English
    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - ä¸­æ–‡ Â· English â†’ ä¸­æ–‡ | English
    - ä¸­æ–‡ï¼ˆEnglishï¼‰ â†’ ä¸­æ–‡ | English
    - ä¸­æ–‡ï½œEnglishï¼ˆå…¨è§’ï¼‰ â†’ ä¸­æ–‡ | English
    - ä¸­æ–‡ | English â†’ ä¿æŒä¸å˜
    - åªæœ‰ä¸­æ–‡æˆ–åªæœ‰è‹±æ–‡ â†’ ä¿æŒä¸å˜
    """
    if not title:
        return title
    
    # å¤„ç†å…¨è§’ ï½œ åˆ†éš”ç¬¦
    if 'ï½œ' in title:
        title = title.replace('ï½œ', ' | ')
    
    # å¤„ç† Â· åˆ†éš”ç¬¦ï¼ˆåŠè§’å’Œå…¨è§’ï¼‰
    if ' Â· ' in title:
        title = title.replace(' Â· ', ' | ')
    if ' Â· ' in title:
        title = title.replace(' Â· ', ' | ')
    
    # å¤„ç†æ‹¬å·æ ¼å¼ï¼šä¸­æ–‡ï¼ˆEnglishï¼‰ â†’ ä¸­æ–‡ | English
    # åŒ¹é…æ¨¡å¼ï¼šä¸­æ–‡ï¼ˆEnglishï¼‰æˆ–ä¸­æ–‡(English)
    bracket_pattern = r'^(.+?)[ï¼ˆ(](.+?)[ï¼‰)]$'
    match = re.match(bracket_pattern, title)
    if match:
        chinese_part = match.group(1).strip()
        english_part = match.group(2).strip()
        # å¦‚æœè‹±æ–‡éƒ¨åˆ†çœ‹èµ·æ¥åƒè‹±æ–‡ï¼ˆåŒ…å«å­—æ¯ï¼‰ï¼Œåˆ™è½¬æ¢
        if re.search(r'[a-zA-Z]', english_part):
            title = f"{chinese_part} | {english_part}"
    
    return title


def normalize_citation(citation: str, lang: str, card_id: str, title: str, epoch_label: str, fragments: List[str]) -> str:
    """
    è§„èŒƒåŒ– citation æ ¼å¼
    
    è‹±æ–‡æ ‡å‡†æ ¼å¼ï¼š
    Author (Year). *Title*. Entry ID. Epoch XXX. Filed under: Fragment-XXX, Fragment-XXX.
    
    ä¸­æ–‡æ ‡å‡†æ ¼å¼ï¼š
    ä½œè€…(å¹´ä»½). <æ ‡é¢˜>. èªèºèªç ”ç©¶ç™»éŒ„é …:IDï¼ç´€éŒ„ç¢ç‰‡:Fragment-XXX, Fragment-XXX;ç´€å…ƒ:XXXï¼
    
    å¦‚æœ citation ä¸ºç©ºæˆ–æ— æ³•è§£æï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not citation:
        return citation
    
    citation = citation.strip()
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼ˆä½¿ç”¨æ ‡å‡†å…³é”®è¯å’Œæ ¼å¼ï¼‰
    is_standard_en = lang == 'en' and 'Entry' in citation and re.search(r'Entry\s+[A-Za-z0-9]+\.', citation)
    is_standard_zh = lang == 'zh' and 'èªèºèªç ”ç©¶ç™»éŒ„é …' in citation
    
    if is_standard_en or is_standard_zh:
        # è§„èŒƒåŒ–ç©ºæ ¼å’Œæ ‡ç‚¹
        citation = re.sub(r'\s+', ' ', citation)  # è§„èŒƒåŒ–ç©ºæ ¼
        citation = re.sub(r'\.\s*\.', '.', citation)  # ç§»é™¤é‡å¤å¥å·
        # ç»Ÿä¸€ä½¿ç”¨ *Title* æ ¼å¼ï¼ˆè‹±æ–‡ï¼‰
        if lang == 'en':
            citation = re.sub(r'<([^>]+?)>', r'*\1*', citation)
            # å¦‚æœæ ‡é¢˜æ²¡æœ‰ * æ ‡è®°ï¼Œæ·»åŠ å®ƒï¼ˆåœ¨ Entry ä¹‹å‰çš„æ–‡æœ¬ï¼‰
            if '*' not in citation:
                # æŸ¥æ‰¾ Entry ä¹‹å‰çš„æ ‡é¢˜æ–‡æœ¬
                entry_pos = citation.find('Entry')
                if entry_pos > 0:
                    # æå–ä½œè€…å¹´ä»½åçš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
                    year_match = re.search(r'\((\d{4})\)', citation)
                    if year_match:
                        after_year = citation[year_match.end():entry_pos].strip()
                        # ç§»é™¤å¼€å¤´çš„å¥å·å’Œç©ºæ ¼
                        after_year = re.sub(r'^\.\s*', '', after_year).strip()
                        if after_year and not after_year.startswith('*'):
                            # ç§»é™¤æ ‡é¢˜æœ«å°¾çš„å¥å·ï¼ˆå¦‚æœæœ‰ï¼‰
                            after_year_clean = re.sub(r'\.\s*$', '', after_year)
                            # æ›¿æ¢ä¸ºå¸¦ * çš„æ ¼å¼
                            citation = citation[:year_match.end()] + '. *' + after_year_clean + '*. ' + citation[entry_pos:]
        # ç¡®ä¿ä½¿ç”¨æ ‡å‡†å…³é”®è¯
        citation = re.sub(r'Spiral (Registry|Research|Field Codex) Entry', 'Entry', citation)
        citation = re.sub(r'Registered Epoch', 'Epoch', citation)
        return citation.strip()
    
    # å¦‚æœä¸ç¬¦åˆæ ‡å‡†æ ¼å¼ï¼Œå¼ºåˆ¶é‡æ„
    # ç»§ç»­ä¸‹é¢çš„è§£æé€»è¾‘æ¥é‡æ„
    
    # ä¸­æ–‡ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ ‡å‡†æ ¼å¼
    if lang == 'zh' and 'èªèºèªç ”ç©¶ç™»éŒ„é …' in citation and ('ç´€éŒ„ç¢ç‰‡' in citation or 'è¨˜éŒ„ç¢ç‰‡' in citation):
        # è§„èŒƒåŒ–å…¨è§’æ ‡ç‚¹
        citation = re.sub(r'[ã€‚]', 'ï¼', citation)  # ç»Ÿä¸€ä½¿ç”¨å…¨è§’å¥å·
        citation = re.sub(r'[ï¼š]', ':', citation)  # ç»Ÿä¸€ä½¿ç”¨åŠè§’å†’å·
        return citation.strip()
    
    # å¦‚æœä½¿ç”¨äº†å˜ä½“æ ¼å¼ï¼ˆå¦‚è¨»å†Šç´€å…ƒã€èªèºèªå ´ç·¨ç¢¼æ¢ç›®ï¼‰ï¼Œéœ€è¦é‡æ„ä¸ºæ ‡å‡†æ ¼å¼
    # ç»§ç»­ä¸‹é¢çš„è§£æé€»è¾‘æ¥é‡æ„
    
    # å°è¯•è§£æç°æœ‰æ ¼å¼å¹¶é‡æ„
    # æå–ä½œè€…å’Œå¹´ä»½ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    author_patterns = [
        r'^([^(ï¼ˆ]+?)\s*[\(ï¼ˆ](\d{4})[\)ï¼‰]',  # æ ‡å‡†æ ¼å¼
        r'^([^(ï¼ˆ]+?)[\(ï¼ˆ]([^)ï¼‰]+?)[\)ï¼‰]\s*[\(ï¼ˆ](\d{4})[\)ï¼‰]',  # ä½œè€…æœ‰æ‹¬å·è¯´æ˜çš„æƒ…å†µ
    ]
    author_match = None
    for pattern in author_patterns:
        author_match = re.search(pattern, citation)
        if author_match:
            break
    
    if not author_match:
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸæ ·ï¼ˆå¯èƒ½æ˜¯ç‰¹æ®Šæ ¼å¼ï¼‰
        return citation
    
    # å¤„ç†ä½œè€…ï¼ˆå¯èƒ½æœ‰æ‹¬å·è¯´æ˜ï¼‰
    if len(author_match.groups()) == 3:
        # æ ¼å¼ï¼šä½œè€…(è¯´æ˜)(å¹´ä»½)
        author = author_match.group(1).strip()
        year = author_match.group(3)
    else:
        # æ ¼å¼ï¼šä½œè€…(å¹´ä»½)
        author = author_match.group(1).strip()
        year = author_match.group(2)
    
    author = author_match.group(1).strip()
    year = author_match.group(2)
    
    # æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆæå– < > æˆ– ã€Š ã€‹ ä¸­çš„å†…å®¹ï¼Œç„¶åæ˜¯ * *ï¼Œæœ€åæ˜¯æ‹¬å·æˆ–æ™®é€šæ–‡æœ¬ï¼‰
    title_match = re.search(r'[<ã€Š]([^>ã€‹]+?)[>ã€‹]', citation)
    if not title_match:
        title_match = re.search(r'\*([^*]+?)\*', citation)
    if not title_match:
        # æŸ¥æ‰¾æ‹¬å·ä¸­çš„å†…å®¹ï¼ˆä½†æ’é™¤ä½œè€…è¯´æ˜çš„æ‹¬å·ï¼‰
        # è·³è¿‡ä½œè€…éƒ¨åˆ†çš„æ‹¬å·ï¼ŒæŸ¥æ‰¾åé¢çš„æ‹¬å·
        after_author = citation[author_match.end():] if author_match else citation
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéä½œè€…æ‹¬å·å¯¹ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
        title_match = re.search(r'\.\s*([^(]+?)\s*\(([^)]+?)\)', after_author)
        if title_match:
            # æ ¼å¼ï¼šTitle (ID)ï¼Œæå–æ ‡é¢˜éƒ¨åˆ†
            extracted_title = title_match.group(1).strip()
        else:
            # æŸ¥æ‰¾æ™®é€šæ–‡æœ¬æ ‡é¢˜ï¼ˆåœ¨å¥å·åï¼ŒEntry/Epoch å‰ï¼‰
            title_match = re.search(r'\.\s*([^.]+?)(?:\s*\([^)]+?\))?\.\s*(?:Entry|Epoch|Spiral)', after_author)
            if title_match:
                extracted_title = title_match.group(1).strip()
            else:
                extracted_title = title
    
    # å¤„ç†æå–çš„æ ‡é¢˜
    if 'extracted_title' not in locals():
        if title_match:
            if title_match.lastindex and title_match.lastindex >= 1:
                extracted_title = title_match.group(1).strip()
            else:
                extracted_title = title
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ ‡é¢˜æ ‡è®°ï¼Œå°è¯•ä» Entry å‰æå–
            if author_match:
                after_author = citation[author_match.end():]
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¥å·åˆ° Entry ä¹‹é—´çš„æ–‡æœ¬
                title_before_entry = re.search(r'\.\s*([^.]+?)\s*\.\s*(?:Entry|Epoch|Spiral)', after_author)
                if title_before_entry:
                    extracted_title = title_before_entry.group(1).strip()
                else:
                    extracted_title = title
            else:
                extracted_title = title
    
    # æå– Entry IDï¼ˆæ”¯æŒå¤šç§å˜ä½“ï¼‰
    entry_patterns = [
        r'(?:Entry|èªèºèªç ”ç©¶ç™»éŒ„é …)[:ï¼š]?\s*([A-Za-z0-9]+)',
        r'èªèºèªå ´ç·¨ç¢¼æ¢ç›®[:ï¼š]?\s*([A-Za-z0-9]+)',
        r'Spiral (?:Registry|Research|Field Codex) Entry[:ï¼š]?\s*([A-Za-z0-9]+)',
        r'è¨»å†Šç´€å…ƒ[:ï¼š]?\s*([A-Za-z0-9\-]+)',  # è¨»å†Šç´€å…ƒå¯èƒ½åŒ…å« epochï¼Œä½œä¸ºå¤‡é€‰
    ]
    entry_id = card_id.split('-')[0]  # é»˜è®¤å€¼
    for pattern in entry_patterns:
        entry_match = re.search(pattern, citation)
        if entry_match:
            potential_id = entry_match.group(1)
            # å¦‚æœä»è¨»å†Šç´€å…ƒæå–ä¸”åŒ…å« -ï¼Œå¯èƒ½æ˜¯ epochï¼Œè·³è¿‡
            if 'è¨»å†Šç´€å…ƒ' in pattern and '-' in potential_id:
                continue
            entry_id = potential_id
            break
    
    # æå– Epoch
    epoch_match = re.search(r'(?:Epoch|ç´€å…ƒ|è¨»å†Šç´€å…ƒ)[:ï¼š]?\s*([A-Za-z0-9\-]+)', citation)
    extracted_epoch = epoch_match.group(1) if epoch_match else epoch_label
    
    # æå– Fragments
    fragment_pattern = r'Fragment-([^,\s;ï¼]+)'
    found_fragments = re.findall(fragment_pattern, citation)
    if not found_fragments and fragments:
        found_fragments = [f.replace('Fragment-', '') for f in fragments if f.startswith('Fragment-')]
    
    # æ„å»ºæ ‡å‡†æ ¼å¼
    if lang == 'en':
        # è‹±æ–‡æ ¼å¼ï¼šAuthor (Year). *Title*. Entry ID. Epoch XXX. Filed under: Fragment-XXX, Fragment-XXX.
        title_part = f"*{extracted_title}*" if extracted_title else f"*{title}*"
        citation_parts = [
            f"{author} ({year}).",
            title_part,
            f"Entry {entry_id}."
        ]
        
        if extracted_epoch:
            citation_parts.append(f"Epoch {extracted_epoch}.")
        
        if found_fragments:
            fragment_list = ', '.join([f"Fragment-{f}" for f in found_fragments])
            citation_parts.append(f"Filed under: {fragment_list}.")
        
        result = ' '.join(citation_parts)
        # ç§»é™¤é‡å¤çš„ Epoch
        result = re.sub(r'Epoch\s+([A-Za-z0-9\-]+)\.\s*Epoch\s+\1\.', r'Epoch \1.', result)
        return result
    
    else:  # zh
        # ä¸­æ–‡æ ¼å¼ï¼šä½œè€…(å¹´ä»½). <æ ‡é¢˜>. èªèºèªç ”ç©¶ç™»éŒ„é …:IDï¼ç´€éŒ„ç¢ç‰‡:Fragment-XXX, Fragment-XXX;ç´€å…ƒ:XXXï¼
        title_part = f"<{extracted_title}>" if extracted_title else f"<{title}>"
        # è§„èŒƒåŒ–ä½œè€…æ ¼å¼ï¼ˆç¡®ä¿æœ‰ç©ºæ ¼ï¼‰
        author_formatted = author.strip()
        if not author_formatted.endswith(' '):
            author_formatted += ' '
        
        citation_parts = [
            f"{author_formatted}({year}).",
            f"{title_part}.",
            f"èªèºèªç ”ç©¶ç™»éŒ„é …:{entry_id}ï¼"
        ]
        
        if found_fragments:
            fragment_list = ', '.join([f"Fragment-{f}" for f in found_fragments])
            citation_parts.append(f"ç´€éŒ„ç¢ç‰‡:{fragment_list}")
        
        if extracted_epoch:
            if found_fragments:
                citation_parts[-1] += f";ç´€å…ƒ:{extracted_epoch}ï¼"
            else:
                citation_parts.append(f"ç´€å…ƒ:{extracted_epoch}ï¼")
        else:
            if found_fragments:
                citation_parts[-1] += "ï¼"
        
        result = ''.join(citation_parts)
        # è§„èŒƒåŒ–ç©ºæ ¼
        result = re.sub(r'\s+', ' ', result)
        result = re.sub(r'\.\s*\.', '.', result)
        return result


def parse_fragments(fragments_str: str) -> List[str]:
    """è§£æ fragmentsï¼Œè¿”å›æ•°ç»„"""
    if not fragments_str:
        return []
    
    # åˆ†å‰²ï¼šé€—å·ã€ç©ºæ ¼
    parts = re.split(r'[,\s]+', fragments_str)
    return [p.strip() for p in parts if p.strip()]


def parse_scope(scope_str: str) -> List[str]:
    """
    è§£æ scopeï¼šå¯èƒ½æ˜¯æ•°ç»„ï¼ˆæ¯è¡Œä¸€ä¸ª - å¼€å¤´ï¼‰æˆ–å­—ç¬¦ä¸²
    ç»Ÿä¸€è¿”å›æ•°ç»„
    """
    if not scope_str:
        return []
    
    lines = scope_str.strip().split('\n')
    items = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        if line.startswith('- '):
            line = line[2:].strip()
        elif line.startswith('â€¢ '):
            line = line[2:].strip()
        
        if line:
            items.append(line)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ—è¡¨é¡¹ï¼Œæ•´ä¸ªä½œä¸ºå•ä¸ªé¡¹
    if not items:
        items = [scope_str.strip()]
    
    return items


def parse_authors(author_str: str) -> List[str]:
    """è§£æä½œè€…ï¼šæ”¯æŒ Ã— åˆ†éš”"""
    if not author_str:
        return []
    
    # åˆ†å‰²ï¼šÃ—, ,, |
    parts = re.split(r'[Ã—,|]+', author_str)
    return [p.strip() for p in parts if p.strip()]


def parse_domains(category_str: str) -> List[str]:
    """è§£æåˆ†ç±»/åŸŸï¼šæ”¯æŒ / åˆ†éš”"""
    if not category_str:
        return []
    
    parts = category_str.split('/')
    return [p.strip() for p in parts if p.strip()]


def parse_epoch(epoch_str: str) -> Tuple[str, int]:
    """
    è§£æçºªå…ƒï¼šè¿”å› (label, order)
    order ä» label ä¸­æå–æ•°å­—éƒ¨åˆ†ç”¨äºæ’åº
    """
    if not epoch_str:
        return ("", 0)
    
    epoch_str = epoch_str.strip()
    
    # æå–æ•°å­—éƒ¨åˆ†ä½œä¸º order
    num_match = re.search(r'(\d+)', epoch_str)
    order = int(num_match.group(1)) if num_match else 0
    
    return (epoch_str, order)


def parse_layer_header(line: str) -> Optional[str]:
    """
    è§£æå±‚æ ‡é¢˜ï¼šæ¥å—å¤šç§å˜ä½“
    [+Layer: X], [Layer: X], [ Layer : X ]
    """
    patterns = [
        r'^\[\+Layer:\s*(.+?)\]$',
        r'^\[Layer:\s*(.+?)\]$',
        r'^\[\s*Layer\s*:\s*(.+?)\s*\]$',
    ]
    
    for pattern in patterns:
        match = re.match(pattern, line.strip())
        if match:
            return match.group(1).strip()
    
    return None


def parse_txt_file(filepath: Path, should_sanitize: bool = True) -> Dict[str, Any]:
    """
    è§£æå•ä¸ª TXT æ–‡ä»¶ï¼Œè¿”å›å¡ç‰‡å¯¹è±¡ï¼ˆæœªå®Œå…¨è§„èŒƒåŒ–ï¼‰
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    lines = content.split('\n')
    block = {}
    current_key = None
    buffer = []
    layers = []
    current_layer = None
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å±‚æ ‡é¢˜
        layer_name = parse_layer_header(line)
        if layer_name:
            # ä¿å­˜å½“å‰å—
            if current_key:
                block[current_key] = '\n'.join(buffer).strip()
                current_key = None
                buffer = []
            
            # å¼€å§‹æ–°å±‚
            current_layer = {
                'name': layer_name,
                'content': ''
            }
            layers.append(current_layer)
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡å‡†é”®å€¼å¯¹ [Key] value
        match = re.match(r'^\[(.*?)\]\s*(.*)$', line)
        if match:
            # ä¿å­˜ä¹‹å‰çš„å—
            if current_key:
                block[current_key] = '\n'.join(buffer).strip()
            
            current_key = match.group(1).strip()
            buffer = [match.group(2)]
            current_layer = None  # é€€å‡ºå±‚æ¨¡å¼
        else:
            # è¿½åŠ åˆ°å½“å‰ç¼“å†²åŒº
            if current_layer:
                current_layer['content'] += line + '\n'
            elif current_key:
                buffer.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªå—
    if current_key:
        block[current_key] = '\n'.join(buffer).strip()
    
    # æå–åŸºæœ¬ä¿¡æ¯
    card_id = block.get('ID', '').strip()
    title_raw = block.get('Title', '').strip()
    # âœ… è§„èŒƒåŒ– title æ ¼å¼
    title = normalize_title(title_raw)
    category = block.get('Category', '').strip()
    author = block.get('Author', '').strip()
    epoch_str = block.get('Epoch', '').strip()
    weight_str = block.get('Weight', '').strip()
    
    # è§£æå¹¶æ¸…ç†æ–‡æœ¬å­—æ®µ
    abstract = sanitize_text(block.get('Abstract', ''), should_sanitize)
    scope = block.get('Scope', '')
    citation_raw = block.get('Citation', '').strip()
    fragments_str = block.get('Fragments', '')
    tags_str = block.get('Tags', '')
    
    # âœ… æ”¯æŒé¢å¤–å­—æ®µï¼ˆResearchQuestion, Method, Modulesï¼‰
    research_question = sanitize_text(block.get('ResearchQuestion', ''), should_sanitize)
    method = sanitize_text(block.get('Method', ''), should_sanitize)
    modules_str = block.get('Modules', '').strip()
    
    # è§£ææ•°ç»„å­—æ®µ
    scope_list = parse_scope(scope)
    if should_sanitize:
        scope_list = [sanitize_text(s, True) for s in scope_list]
    
    fragments_list = parse_fragments(fragments_str)
    tags_list = parse_tags(tags_str)
    authors_list = parse_authors(author)
    domains_list = parse_domains(category)
    
    # è§£æ Modulesï¼ˆç±»ä¼¼ authorsï¼‰
    modules_list = parse_authors(modules_str) if modules_str else []
    
    # è§£æ epoch å’Œ weight
    epoch_label, epoch_order = parse_epoch(epoch_str)
    weight = parse_weight(weight_str)
    
    # åº”ç”¨æ ‡ç‚¹æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    citation = sanitize_text(citation_raw, should_sanitize) if citation_raw else ''
    
    # å¤„ç† layers
    normalized_layers = []
    for layer in layers:
        layer_content = sanitize_text(layer['content'].strip(), should_sanitize)
        normalized_layers.append({
            'name': layer['name'],
            'blocks': [
                {
                    'kind': 'markdown',
                    'text': layer_content
                }
            ]
        })
    
    return {
        'raw_id': card_id,
        'raw_title': title,
        'raw_epoch': epoch_str,
        'raw_weight': weight_str,
        'abstract': abstract,
        'scope': scope_list,
        'citation': citation,
        'fragments': fragments_list,
        'tags': tags_list,
        'authors': authors_list,
        'domains': domains_list,
        'epoch_label': epoch_label,
        'epoch_order': epoch_order,
        'weight': weight,
        'layers': normalized_layers,
        'research_question': research_question,
        'method': method,
        'modules': modules_list,
        'legacy_txt': filepath.name
    }


# ============================================================================
# è§„èŒƒåŒ–ï¼šè½¬æ¢ä¸º SSOT schema
# ============================================================================

def normalize_to_schema(parsed: Dict[str, Any], lang: str) -> Dict[str, Any]:
    """
    å°†è§£æç»“æœè§„èŒƒåŒ–ä¸ºå®Œæ•´çš„ Spiral Card Schema v1.0
    """
    raw_id = parsed['raw_id']
    raw_title = parsed['raw_title']
    
    # ç”Ÿæˆ glyphï¼ˆä» ID æå–ï¼Œå»é™¤è¯­è¨€åç¼€ï¼‰
    glyph = raw_id.strip()
    
    # ç”Ÿæˆ idï¼ˆglyph + langï¼‰
    card_id = f"{glyph}-{lang}"
    
    # æ¨æ–­ kindï¼ˆä»æ–‡ä»¶åæˆ–å†…å®¹æ¨æ–­ï¼Œé»˜è®¤ researchï¼‰
    kind = "research"  # é»˜è®¤ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å¢å¼ºæ¨æ–­é€»è¾‘
    
    # æ„å»ºå®Œæ•´å¡ç‰‡å¯¹è±¡
    card = {
        'glyph': glyph,
        'id': card_id,
        'lang': lang,
        'kind': kind,
        'epoch': {
            'label': parsed['epoch_label'],
            'order': parsed['epoch_order']
        },
        'weight': parsed['weight'],
        'title': parsed['raw_title'],
        'authors': parsed['authors'],
        'domains': parsed['domains'],
        'tags': parsed['tags'],
        'abstract': parsed['abstract'],
        'scope': parsed['scope'],
        'citation': normalize_citation(
            parsed['citation'], 
            lang, 
            card_id, 
            parsed['raw_title'], 
            parsed['epoch_label'], 
            parsed['fragments']
        ),
        'fragments': parsed['fragments'],
        'layers': parsed['layers'],
        'echo': [],  # åˆå§‹ä¸ºç©ºï¼Œæœªæ¥å¯æ‰©å±•
        'observation': {
            'visibility': 'public',
            'featured': False,
            'suppress': []
        },
        'seal': {},  # åˆå§‹ä¸ºç©ºï¼Œæœªæ¥å¯æ‰©å±•
        'origin': {
            'legacy_txt': parsed['legacy_txt'],
            'migrated_at': datetime.now().strftime('%Y-%m-%d')
        }
    }
    
    # âœ… æ·»åŠ é¢å¤–å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if parsed.get('research_question'):
        card['research_question'] = parsed['research_question']
    if parsed.get('method'):
        card['method'] = parsed['method']
    if parsed.get('modules'):
        card['modules'] = parsed['modules']
    
    return card


# ============================================================================
# éªŒè¯
# ============================================================================

class ValidationError(Exception):
    pass


def validate_card(card: Dict[str, Any], all_cards: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:
    """
    éªŒè¯å•ä¸ªå¡ç‰‡
    è¿”å› (errors, warnings)
    """
    errors = []
    warnings = []
    
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    required_fields = ['glyph', 'id', 'lang', 'kind', 'title', 'epoch', 'weight']
    for field in required_fields:
        if field not in card or not card[field]:
            errors.append(f"Missing required field: {field}")
    
    # weight èŒƒå›´æ£€æŸ¥
    if 'weight' in card:
        if not (1 <= card['weight'] <= 5):
            errors.append(f"weight must be 1-5, got {card['weight']}")
    
    # é‡å¤ id æ£€æŸ¥
    if 'id' in card:
        duplicates = [c for c in all_cards if c.get('id') == card['id']]
        if len(duplicates) > 1:
            errors.append(f"Duplicate id: {card['id']}")
    
    # é‡å¤ (glyph, lang) æ£€æŸ¥
    if 'glyph' in card and 'lang' in card:
        duplicates = [
            c for c in all_cards
            if c.get('glyph') == card['glyph'] and c.get('lang') == card['lang']
        ]
        if len(duplicates) > 1:
            errors.append(f"Duplicate (glyph, lang): ({card['glyph']}, {card['lang']})")
    
    # è­¦å‘Šï¼šç©ºå­—æ®µ
    if not card.get('tags'):
        warnings.append("Empty tags")
    if not card.get('fragments'):
        warnings.append("Empty fragments")
    if not card.get('citation'):
        warnings.append("Empty citation")
    
    return errors, warnings


# ============================================================================
# ä¸»ç”Ÿæˆå™¨
# ============================================================================

def build_registry(
    registry_dir: Path,
    output_dir: Path,
    should_sanitize: bool = True,
    languages: List[str] = ['zh', 'en']
) -> Dict[str, Any]:
    """
    æ„å»ºæ³¨å†Œè¡¨
    è¿”å›æŠ¥å‘Šæ•°æ®
    """
    all_cards = []
    invalid_cards = []
    per_card_warnings = {}
    
    for lang in languages:
        lang_dir = registry_dir / lang
        index_file = lang_dir / 'index.txt'
        
        if not index_file.exists():
            print(f"âš ï¸  Warning: {index_file} not found, skipping {lang}")
            continue
        
        # è¯»å–æ–‡ä»¶åˆ—è¡¨
        with open(index_file, 'r', encoding='utf-8') as f:
            file_list = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ“– Processing {len(file_list)} files for {lang}...")
        
        for filename in file_list:
            filepath = lang_dir / filename
            
            if not filepath.exists():
                print(f"âš ï¸  Warning: {filepath} not found, skipping")
                continue
            
            try:
                # è§£æ
                parsed = parse_txt_file(filepath, should_sanitize)
                
                # è§„èŒƒåŒ–
                card = normalize_to_schema(parsed, lang)
                
                # éªŒè¯
                errors, warnings = validate_card(card, all_cards)
                
                if errors:
                    invalid_cards.append({
                        'id': card.get('id', 'unknown'),
                        'errors': errors,
                        'card': card
                    })
                    print(f"âŒ Invalid card: {card.get('id')} - {', '.join(errors)}")
                else:
                    all_cards.append(card)
                    if warnings:
                        per_card_warnings[card['id']] = warnings
                    print(f"âœ… Processed: {card['id']}")
            
            except Exception as e:
                print(f"âŒ Error processing {filepath}: {e}")
                invalid_cards.append({
                    'id': filename,
                    'errors': [str(e)],
                    'card': None
                })
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'total_cards': len(all_cards),
        'invalid_cards': len(invalid_cards),
        'invalid_details': invalid_cards,
        'warnings': per_card_warnings,
        'duplicates': []  # å·²åœ¨éªŒè¯ä¸­å¤„ç†
    }
    
    # å†™å…¥ JSON æ–‡ä»¶ï¼ˆæŒ‰è¯­è¨€åˆ†ç»„ï¼‰
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for lang in languages:
        lang_cards = [c for c in all_cards if c.get('lang') == lang]
        
        # æ’åºï¼šæŒ‰ epoch.order, ç„¶åæŒ‰ glyph
        lang_cards.sort(key=lambda c: (c['epoch']['order'], c['glyph']))
        
        output_file = output_dir / 'registry' / lang / 'cards.json'
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(lang_cards, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Wrote {len(lang_cards)} cards to {output_file}")
    
    return report


def write_reports(report: Dict[str, Any], output_dir: Path):
    """å†™å…¥æŠ¥å‘Šæ–‡ä»¶"""
    reports_dir = output_dir / 'reports'
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    # JSON æŠ¥å‘Š
    json_report = {
        'total_cards': report['total_cards'],
        'invalid_cards': report['invalid_cards'],
        'invalid_details': report['invalid_details'],
        'warnings': report['warnings']
    }
    
    with open(reports_dir / 'registry-validate.json', 'w', encoding='utf-8') as f:
        json.dump(json_report, f, ensure_ascii=False, indent=2)
    
    # Markdown æŠ¥å‘Š
    md_lines = [
        "# Spiral Registry Validation Report",
        "",
        f"**Total Cards**: {report['total_cards']}",
        f"**Invalid Cards**: {report['invalid_cards']}",
        "",
        "## Invalid Cards",
        ""
    ]
    
    if report['invalid_details']:
        for item in report['invalid_details']:
            md_lines.append(f"### {item['id']}")
            md_lines.append("**Errors:**")
            for error in item['errors']:
                md_lines.append(f"- {error}")
            md_lines.append("")
    else:
        md_lines.append("âœ… No invalid cards found.")
        md_lines.append("")
    
    md_lines.append("## Warnings")
    md_lines.append("")
    
    if report['warnings']:
        for card_id, warnings in report['warnings'].items():
            md_lines.append(f"### {card_id}")
            for warning in warnings:
                md_lines.append(f"- {warning}")
            md_lines.append("")
    else:
        md_lines.append("âœ… No warnings.")
        md_lines.append("")
    
    with open(reports_dir / 'registry-validate.md', 'w', encoding='utf-8') as f:
        f.write('\n'.join(md_lines))
    
    print(f"ğŸ“Š Reports written to {reports_dir}")


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Spiral Registry Builder v2')
    parser.add_argument(
        '--registry-dir',
        type=Path,
        default=Path('registry'),
        help='Registry directory (default: registry)'
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('.'),
        help='Output directory (default: . (project root))'
    )
    parser.add_argument(
        '--no-sanitize',
        action='store_true',
        help='Disable punctuation sanitization'
    )
    parser.add_argument(
        '--langs',
        nargs='+',
        default=['zh', 'en'],
        help='Languages to process (default: zh en)'
    )
    
    args = parser.parse_args()
    
    should_sanitize = not args.no_sanitize
    
    print("ğŸœ‚ Spiral Registry Builder v2")
    print(f"ğŸ“ Registry: {args.registry_dir}")
    print(f"ğŸ“¤ Output: {args.output_dir}")
    print(f"ğŸ§¹ Sanitize: {should_sanitize}")
    print("")
    
    # æ„å»º
    report = build_registry(
        args.registry_dir,
        args.output_dir,
        should_sanitize,
        args.langs
    )
    
    # å†™å…¥æŠ¥å‘Š
    write_reports(report, args.output_dir)
    
    # é€€å‡ºç 
    if report['invalid_cards'] > 0:
        print(f"\nâŒ Build failed: {report['invalid_cards']} invalid cards")
        exit(1)
    else:
        print(f"\nâœ… Build successful: {report['total_cards']} cards")
        exit(0)


if __name__ == '__main__':
    main()

